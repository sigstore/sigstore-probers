name: Reusable Sigstore Prober Workflow

on:
  workflow_call:
    secrets:
      PAGERDUTY_INTEGRATION_KEY:
        description: 'Integration key for PagerDuty'
        required: true
    inputs:
      rekor_v2_url:
        required: false
        type: string
        description: 'Rekor v2 URL'
      rekor_v2_public_key:
        required: false
        type: string
        description: 'Rekor v2 Public Key'
      enable_staging:
        required: false
        type: boolean
      tuf_repo:
        required: false
        type: string
        description: 'TUF Repo'
      tuf_root_path:
        required: false
        type: string
        default: '.github/assets/sigstore.root.json'
        description: "path to the tuf root"
      triggerPagerDutyTest:
        description: 'Trigger PagerDuty test message'
        required: false
        type: string
      github_issues_repo:
        required: false
        type: string
        default: 'sigstore/sigstore-probers'
        description: 'GitHub repo to file alert issues under'

permissions: {}

jobs:

  sigstore-probe:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    permissions:
      contents: read
      id-token: write

    outputs:
      sigstore_probe: ${{ steps.msg.outputs.sigstore_probe }}
    steps:
      - name: Extract prober binary
        run: |
          docker pull ghcr.io/sigstore/sigstore-probers:latest
          # the last argument in the next command is not used, it is required because the container doesn't have a default command
          docker create --name binaries ghcr.io/sigstore/sigstore-probers /usr/local/bin/prober
          docker cp binaries:/usr/local/bin/prober /usr/local/bin/

      # Make sure rekor is up and we can get root info
      - name: Run prober
        env:
          DEBUG: 1
        uses: nick-fields/retry@ce71cc2ab81d554ebbe88c79ab5975992d79ba08 # v3.0.2
        with:
          timeout_minutes: 3
          max_attempts: 3
          retry_wait_seconds: 60
          retry_on: error
          command: prober --one-time ${{ inputs.enable_staging && '--staging' || '' }}

      - name: Set messages
        id: msg
        if: success() || failure()
        run: |
          echo "sigstore_probe=good" >> $GITHUB_OUTPUT
          if [ "${{ job.status }}" == 'failure' ]; then echo "sigstore_probe=failure" >> $GITHUB_OUTPUT; fi


  root-probe:
    timeout-minutes: 10
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    outputs:
      root_state: ${{ steps.msg.outputs.root_state }}
      dedup_key: ${{ steps.msg.outputs.dedup_key }}
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          persist-credentials: false

      - name: Set initial root.json
        run: cp "${{ inputs.tuf_root_path }}" ./root.json

      - name: Verify TUF repository state
        uses: theupdateframework/tuf-on-ci/actions/test-repository@0ef66318d40656f421781f28df93de9c69ccb9ba # v0.18.0
        with:
          metadata_url: ${{ inputs.tuf_repo }}
          valid_days: 2
          offline_valid_days: 15
          metadata_dir: tuf/
          compare_source: false

      - name: Set messages
        id: msg
        if: success() || failure()
        run: |
          # calculate a dedup key based on all TUF metadata the test client saw
          echo "dedup_key=$(sha256sum tuf/*/* | sha256sum | cut -d " " -f 1)" >> $GITHUB_OUTPUT

          echo "root_state=good" >> $GITHUB_OUTPUT
          if [ "${{ job.status }}" == 'failure' ]; then echo "root_state=failure" >> $GITHUB_OUTPUT; fi

  rekor-fulcio-e2e:
    strategy:
      matrix:
        version: [v1, v2]
      fail-fast: false
    name: rekor-${{ matrix.version }}-fulcio-e2e
    timeout-minutes: 10
    permissions:
      id-token: write
      contents: read
    env:
      COSIGN_YES: "true"
      IMAGE: localhost:1338/image:${{ github.sha }}-${{ github.run_id }}-${{ matrix.version }}
      IDENTITY_REGEX: ${{ github.server_url }}/${{ github.repository }}/.github/workflows/reusable-prober.yml@refs/.*
      TIMESTAMP_VERIFY_FLAG: ${{ matrix.version == 'v2' && '--use-signed-timestamps' || '' }}
    runs-on: ubuntu-latest
    outputs:
      result: ${{ job.status }}
      skip_pagerduty: ${{ steps.set-skip-pagerduty.outputs.skip_pagerduty }}
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          persist-credentials: false

      - name: Validate Rekor v2 inputs
        if: matrix.version == 'v2'
        run: |
          if [[ ('${{ inputs.rekor_v2_url }}' && ! '${{ inputs.rekor_v2_public_key }}') || (! '${{ inputs.rekor_v2_url }}' && '${{ inputs.rekor_v2_public_key }}') ]]; then
            echo "Error: rekor_v2_url and rekor_v2_public_key must be provided together."
            exit 1
          fi

      # This server is often down, resulting in a lot of flaky probers
      # If the server is down, and this step fails, we don't alert PagerDuty
      - name: Confirm Github OIDC Server is Available
        continue-on-error: true
        run: |
          curl -H "Authorization: Bearer $ACTIONS_ID_TOKEN_REQUEST_TOKEN" $ACTIONS_ID_TOKEN_REQUEST_URL&audience=sigstore

      # Since the server is down, we want to ignore the failure in this workflow
      # and skip paging PagerDuty
      - name: Set skip_pagerduty outputs
        id: set-skip-pagerduty
        if: failure()
        run: |
          echo "skip_pagerduty=true" >> $GITHUB_OUTPUT

      - name: Extract relevant binaries
        run: |
          docker pull ghcr.io/sigstore/sigstore-probers:latest
          # the last argument in the next command is not used, it is required because the container doesn't have a default command
          docker create --name binaries ghcr.io/sigstore/sigstore-probers /usr/local/bin/crane
          docker cp binaries:/usr/local/bin/crane /usr/local/bin/
          docker cp binaries:/usr/local/bin/cosign /usr/local/bin/

      # Setup the registry on port 1338
      - run: |
          PORT=1338 crane registry serve &

      # this pulls a container from GHCR to avoid docker.io rate limiting and minimize network flake risk
      - name: Build and copy a container image
        run: |
          docker pull ghcr.io/linuxcontainers/alpine
          docker tag ghcr.io/linuxcontainers/alpine ${IMAGE}
          docker push ${IMAGE}

      - name: Initialize TUF root
        run: |
          for i in {1..5}
          do
            if cosign initialize --mirror=${{ inputs.tuf_repo }} --root=${{ inputs.tuf_root_path }}; then
              echo "Successfully initialized" && exit 0
            else
              echo "Failed to initialize" && sleep 10
            fi
          done
          exit 1

      - name: List cosign TUF root contents
        run: |
          TUF_TARGETS_DIR=~/.sigstore/root/$(echo ${{ inputs.tuf_repo }} | sed -e 's#^https://##' -e 's#/$##')/targets
          echo "These are the contents of the cosign TUF root: "
          ls $TUF_TARGETS_DIR

      - name: Prepare signing config and trusted root
        run: |
          TUF_TARGETS_DIR=~/.sigstore/root/$(echo ${{ inputs.tuf_repo }} | sed -e 's#^https://##' -e 's#/$##')/targets
          cp ${TUF_TARGETS_DIR}/signing_config.v0.2.json ./signing_config.v0.2.json
          cp ${TUF_TARGETS_DIR}/trusted_root.json ./trusted_root.json

      - name: Configure signing config for Rekor v1
        if: matrix.version == 'v1'
        run: |
          # Filter the signing config to only use Rekor v1 services.
          jq '.rekorTlogUrls |= map(select(.majorApiVersion == 1))' signing_config.v0.2.json > signing_config.v0.2.json.tmp && mv signing_config.v0.2.json.tmp signing_config.v0.2.json

      - name: Configure signing config and trusted root for Rekor v2
        if: matrix.version == 'v2'
        run: |
          # If a Rekor v2 URL is provided, use it and the provided public key.
          if [[ -n "${{ inputs.rekor_v2_url }}" ]]; then
            jq --arg url "${{ inputs.rekor_v2_url }}" '.rekorTlogUrls = [{"url": $url, "majorApiVersion": 2}]' signing_config.v0.2.json > signing_config.v0.2.json.tmp && mv signing_config.v0.2.json.tmp signing_config.v0.2.json
            # If a Rekor service for the URL is present in the trusted root, error out.
            if jq -e --arg url "${{ inputs.rekor_v2_url }}" '.tlogs[] | select(.baseUrl == $url)' trusted_root.json > /dev/null; then
              echo "Error: Rekor service URL ${{ inputs.rekor_v2_url }} already exists in the trusted root."
              exit 1
            else
              # Calculate the key ID according to the C2SP spec for ED25519 signatures: https://github.com/sigstore/rekor-tiles/blob/main/CLIENTS.md#trustedroot-lookup-by-checkpoint-key-id-rather-than-log-id
              START_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
              KEY_NAME=$(echo "${{ inputs.rekor_v2_url }}" | sed -e 's#^https://##' -e 's#/$##')
              # tail is used here to strip the 12-byte ASN.1 header from the decoded public key
              KEY_ID=$( (printf "%s" "$KEY_NAME"; printf "\x0a\x01"; echo -n "${{ inputs.rekor_v2_public_key }}" | base64 -d | tail -c+13) | sha256sum | head -c 64 | xxd -r -p | base64 -w0)
              jq --arg url "${{ inputs.rekor_v2_url }}" --arg key "${{ inputs.rekor_v2_public_key }}" --arg key_id "$KEY_ID" --arg start_time "$START_TIME" '.tlogs += [{"baseUrl": $url, "hashAlgorithm": "SHA2_256", "publicKey": {"rawBytes": $key, "keyDetails": "PKIX_ED25519", "validFor": {"start": $start_time}}, "logId": {"keyId": $key_id}}]' trusted_root.json > trusted_root.json.tmp && mv trusted_root.json.tmp trusted_root.json
            fi
          # If no inputs are provided, use the default behavior for the environment.
          elif [[ "${{ inputs.enable_staging }}" == "false" ]]; then
            # Prod: hardcode the latest Rekor v2 shard URL until Rekor v2 is available in the prod signing config.
            REKOR_V2_URL="https://log2025-1.rekor.sigstore.dev"
            jq --arg url "$REKOR_V2_URL" '.rekorTlogUrls = [{"url": $url, "majorApiVersion": 2}]' signing_config.v0.2.json > signing_config.v0.2.json.tmp && mv signing_config.v0.2.json.tmp signing_config.v0.2.json
          else
            # Staging: filter the signing config to only use Rekor v2 services.
            jq '.rekorTlogUrls |= map(select(.majorApiVersion == 2))' signing_config.v0.2.json > signing_config.v0.2.json.tmp && mv signing_config.v0.2.json.tmp signing_config.v0.2.json
          fi

      - name: Sign and verify the image
        run: |
          cosign sign --yes ${IMAGE} --signing-config signing_config.v0.2.json --trusted-root trusted_root.json --oidc-provider github-actions --new-bundle-format
          cosign verify ${IMAGE} --trusted-root trusted_root.json --certificate-oidc-issuer=https://token.actions.githubusercontent.com --certificate-identity-regexp="$IDENTITY_REGEX" --new-bundle-format ${{ env.TIMESTAMP_VERIFY_FLAG }}

      - name: Generate and upload attestation
        run: |
          cosign attest --predicate ./prober/attestation.json --type slsaprovenance --signing-config signing_config.v0.2.json --trusted-root trusted_root.json ${IMAGE} --new-bundle-format
          cosign verify-attestation --type=slsaprovenance ${IMAGE} --trusted-root trusted_root.json --certificate-oidc-issuer=https://token.actions.githubusercontent.com --certificate-identity-regexp="$IDENTITY_REGEX" --new-bundle-format ${{ env.TIMESTAMP_VERIFY_FLAG }}

      - name: Read entries from all Rekor v1 shards
        if: matrix.version == 'v1'
        run: |
          # Assume the signing config contains a single Rekor v1 URL
          set -e
          REKOR_URL=$(jq -r '.rekorTlogUrls[] | select(.majorApiVersion == 1) | .url' signing_config.v0.2.json)
          # get shard information from Rekor
          response=$(curl -fs $REKOR_URL/api/v1/log) || {
            echo "Fetching loginfo from $REKOR_URL failed, exiting..."; exit 1;
          }

          # shard_sizes[0] is the size of the active shard, shard_sizes [1...n] are the sizes of the inactive shards
          mapfile -t SHARD_SIZES < <(echo ${response} | jq '.. | objects | .treeSize')

          result=()
          offset=0
          # Check if there are any inactive shards returned (len >= 2)
          if (( ${#SHARD_SIZES[@]} >= 2 )); then
            for (( i = 1; i < ${#SHARD_SIZES[@]}; i++ )); do
              lower_bound=${offset}
              upper_bound=$((SHARD_SIZES[i]-1))
              random_number=$(( (RANDOM % (upper_bound - lower_bound + 1)) + lower_bound ))
              result+=($random_number)
              offset=$((upper_bound+offset+1))
            done
          fi

          # now handle the active shard
          if (( ${SHARD_SIZES[0]} > 0 )); then
            lower_bound=${offset}
            upper_bound=$((SHARD_SIZES[0]-1))
            random_number=$(( (RANDOM % (upper_bound - lower_bound + 1)) + lower_bound ))
            result+=($random_number)
          fi

          for index in "${result[@]}"; do
            echo -n "Fetching index $index from $REKOR_URL ... "
            entry=$(curl -fs $REKOR_URL/api/v1/log/entries?logIndex=$index) || {
              echo "failed!"; exit 1;
            }
            echo
            echo $entry | jq .
          done

      - name: Set messages
        id: msg
        if: success() || failure()
        run: |
          echo "result=${{ job.status }}" >> $GITHUB_OUTPUT

  process-e2e-results:
    if: always()
    runs-on: ubuntu-latest
    needs: rekor-fulcio-e2e
    # This job must have steps to be valid, even if it only computes outputs.
    steps:
      - name: Aggregate matrix results
        run: echo "Aggregating results from the rekor-fulcio-e2e matrix."
    outputs:
      # Correctly check if any matrix job failed or was cancelled.
      result: ${{ (contains(needs.rekor-fulcio-e2e.*.result, 'failure') || contains(needs.rekor-fulcio-e2e.*.result, 'cancelled')) && 'failure' || 'success' }}
      details: ${{ toJSON(needs.rekor-fulcio-e2e) }}
      skip_pagerduty: ${{ contains(needs.rekor-fulcio-e2e.*.outputs.skip_pagerduty, 'true') }}
      v1_result: ${{ needs.rekor-fulcio-e2e.v1.result || 'skipped' }}
      v2_result: ${{ needs.rekor-fulcio-e2e.v2.result || 'skipped' }}

  compute-summary-msg:
    runs-on: ubuntu-latest
    outputs:
      summary: ${{ steps.msg.outputs.summary }}
      group: ${{ steps.msg.outputs.group }}
    steps:
      - name: Set messages
        id: msg
        if: success() || failure()
        run: |
          if [ "${{ inputs.triggerPagerDutyTest }}" == "true" ]; then
            echo "summary=Test Notification" >> $GITHUB_OUTPUT
          else
            echo "summary=Prober Failed" >> $GITHUB_OUTPUT
          fi
          echo "group=production" >> $GITHUB_OUTPUT
          if [ ${{ inputs.enable_staging }} == 'true' ]; then
            echo "group=staging" >> $GITHUB_OUTPUT;
          fi

  pagerduty-notification:
    if: github.event.inputs.triggerPagerDutyTest=='true' || (failure() && needs.process-e2e-results.outputs.skip_pagerduty != 'true')
    needs: [sigstore-probe, root-probe, rekor-fulcio-e2e, compute-summary-msg, process-e2e-results]
    uses: ./.github/workflows/reusable-pager.yml
    permissions:
      contents: read
    secrets:
      PAGERDUTY_INTEGRATION_KEY: ${{ secrets.PAGERDUTY_INTEGRATION_KEY }}
    with:
      summary: ${{ needs.compute-summary-msg.outputs.summary }}
      component: "health prober"
      group: ${{ needs.compute-summary-msg.outputs.group }}
      dedup_key: "${{ needs.root-probe.outputs.dedup_key }}-${{ needs.process-e2e-results.outputs.result }}"
      details: >
        {
          "Environment": "${{ needs.compute-summary-msg.outputs.group }}",
          "Failure URL": "https://github.com/sigstore/sigstore-probers/actions/runs/${{ github.run_id }}",
          "Commit": "${{ github.sha }}",
          "Prober": "${{ needs.sigstore-probe.outputs.sigstore_probe }}",
          "GCS Root": "${{ needs.root-probe.outputs.root_state }}",
          "Rekor/Fulcio E2E (v1)": "${{ needs.process-e2e-results.outputs.v1_result }}",
          "Rekor/Fulcio E2E (v2)": "${{ needs.process-e2e-results.outputs.v2_result }}"
        }
      links: >
        [
          {
            "href": "https://github.com/sigstore/public-good-instance/blob/main/playbooks/alerting/alerts/k8s-api-endpoint-prober.md",
            "text": "Prober Failure Playbook"
          },
          {
            "href": "https://github.com/sigstore/public-good-instance/blob/main/playbooks/alerting/tuf.md",
            "text": "Expiring TUF Metadata Playbook"
          }
        ]

  github-issue:
    if: always() && (needs.sigstore-probe.result == 'failure' || needs.root-probe.result == 'failure' || needs.process-e2e-results.result == 'failure')
    runs-on: ubuntu-latest
    needs: [sigstore-probe, root-probe, rekor-fulcio-e2e, compute-summary-msg, process-e2e-results]
    permissions:
      issues: write
    steps:
      - env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cat << 'EOF' | gh issue create --repo ${{ inputs.github_issues_repo }} --title "[ALERT][${{ needs.compute-summary-msg.outputs.group }}] Prober Failed" --label "alert,needs-assignee,oncall,${{ inputs.enable_staging && 'env:staging' || 'env:production' }}" -F -
          Environment: ${{ needs.compute-summary-msg.outputs.group }}
          Failure URL: https://github.com/sigstore/sigstore-probers/actions/runs/${{ github.run_id }}
          Commit: ${{ github.sha }}
          Prober: ${{ needs.sigstore-probe.outputs.sigstore_probe }}
          GCS Root: ${{ needs.root-probe.outputs.root_state }}
          Rekor/Fulcio E2E (v1): ${{ needs.process-e2e-results.outputs.v1_result }}
          Rekor/Fulcio E2E (v2): ${{ needs.process-e2e-results.outputs.v2_result }}
          EOF
